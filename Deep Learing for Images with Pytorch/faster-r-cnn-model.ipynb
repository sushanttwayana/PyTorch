{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4748798,"sourceType":"datasetVersion","datasetId":2716794}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torch.optim as optim\nimport torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T10:33:28.517841Z","iopub.execute_input":"2024-07-01T10:33:28.518310Z","iopub.status.idle":"2024-07-01T10:33:28.525175Z","shell.execute_reply.started":"2024-07-01T10:33:28.518276Z","shell.execute_reply":"2024-07-01T10:33:28.523853Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Anchor generator\n\nYour team is developing object detection models based on the Faster R-CNN architecture and using pre-trained backbones. Your task is to create anchor boxes to serve as reference bounding boxes for proposing potential object regions.\n\nYou will create 9 standard anchors (3 box sizes and 3 aspect ratios).\n\n* Import AnchorGenerator from torchvision.models.detection.rpn.\n* Configure anchor sizes with 3 values: ((32, 64, 128),).\n* Configure aspect ratio with 3 values `((0.5, 1.0, 2.0),).\n* Instantiate AnchorGenerator with anchor_sizes and aspect_ratios.","metadata":{}},{"cell_type":"code","source":"# Import AnchorGenerator\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# Configure anchor size\nanchor_sizes = ((32, 64, 128),)\n\n# Configure aspect ratio\naspect_ratios = ((0.5, 1.0, 2.0),)\n\n# Instantiate AnchorGenerator\nrpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:26:20.996066Z","iopub.execute_input":"2024-07-01T10:26:20.997253Z","iopub.status.idle":"2024-07-01T10:26:21.100286Z","shell.execute_reply.started":"2024-07-01T10:26:20.997192Z","shell.execute_reply":"2024-07-01T10:26:21.098685Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(rpn_anchor_generator)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:26:32.984075Z","iopub.execute_input":"2024-07-01T10:26:32.984596Z","iopub.status.idle":"2024-07-01T10:26:32.992285Z","shell.execute_reply.started":"2024-07-01T10:26:32.984556Z","shell.execute_reply":"2024-07-01T10:26:32.990773Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"AnchorGenerator()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The AnchorGenerator can generate anchor boxes for multiple sizes and for different scales in the image.","metadata":{}},{"cell_type":"markdown","source":"# Faster R-CNN model\n\nYour next task is to build a Faster R-CNN model that can detect objects of different sizes in an image. For this task, you will be using a handy class MultiScaleRoIAlign() from torchvision.ops.\n\n* Import MultiScaleRoIAlign from torchvision.ops.\n* Instantiate the RoI pooler using MultiScaleRoIAlign with featmap_names set to [\"0\"], output_size to 7, and sampling_ratio to 2.\n* Create the Faster R-CNN model passing it the backbone, num_class for a binary classification, anchor_generator, and roi_pooler.","metadata":{}},{"cell_type":"code","source":"from torchvision.models.detection import FasterRCNN\n# Import MultiScaleRoIAlign\nfrom torchvision.ops import MultiScaleRoIAlign\n\n# Instantiate RoI pooler\nroi_pooler = MultiScaleRoIAlign(\n    featmap_names = [\"0\"],\n    output_size = 7,\n    sampling_ratio = 2,\n    )\n\nmobilenet = torchvision.models.mobilenet_v2(weights=\"DEFAULT\")\nbackbone = nn.Sequential(*list(mobilenet.features.children()))\nbackbone.out_channels = 1280\n\n# Create Faster R-CNN model\nmodel = FasterRCNN(\n    backbone=backbone,\n    num_classes=2,\n    anchor_generator=rpn_anchor_generator,\n    box_roi_pool=roi_pooler,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:33:52.292444Z","iopub.execute_input":"2024-07-01T10:33:52.292931Z","iopub.status.idle":"2024-07-01T10:33:53.612642Z","shell.execute_reply.started":"2024-07-01T10:33:52.292894Z","shell.execute_reply":"2024-07-01T10:33:53.611359Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Define losses for RPN and R-CNN\n\nYou are planning to train an object detection model that utilizes both the RPN and R-CNN components. To be able to train it, you will need to define the loss function for each component.\n\nYou remember that the RPN component classifies whether a region contains an object and predicts the bounding box coordinates for the proposed regions.The R-CNN component classifies the object into one of multiple classes while also predicting the final bounding box coordinates.\n\n* Define the RPN classification loss function and assign it to rpn_cls_criterion.\n* Define the RPN regression loss function and assign it to rpn_reg_criterion.\n* Define the R-CNN classification loss function and assign it to rcnn_cls_criterion.\n* Define the R-CNN regression loss function using and assign it to rcnn_reg_criterion.\n\n","metadata":{}},{"cell_type":"code","source":"# Implement the RPN classification loss function\nrpn_cls_criterion = nn.BCEWithLogitsLoss()\n\n# Implement the RPN regression loss function\nrpn_reg_criterion = nn.MSELoss()\n\n# Implement the R-CNN classification Loss function\nrcnn_cls_criterion = nn.CrossEntropyLoss()\n\n# Implement the R-CNN regression loss function\nrcnn_reg_criterion = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:37:46.149320Z","iopub.execute_input":"2024-07-01T10:37:46.149927Z","iopub.status.idle":"2024-07-01T10:37:46.157885Z","shell.execute_reply.started":"2024-07-01T10:37:46.149884Z","shell.execute_reply":"2024-07-01T10:37:46.156469Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Using the correct loss functions is crucial for training machine learning models effectively and it ensures that the model updates its parameters correctly.","metadata":{}},{"cell_type":"markdown","source":"**Prepare A Dataset**\n\nFirst, create a custom dataset class that can load images and annotations. Assuming you have annotations for each image, you'll need to parse these annotations and return them along with the images.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\n\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir)]\n        # Assuming you have a way to get annotations for each image\n        self.annotations = self.load_annotations()\n\n    def load_annotations(self):\n        # Implement this function to load annotations for your dataset\n        # This should return a dictionary where keys are image filenames and values are annotations\n        annotations = {}\n        # Example: annotations['image1.jpg'] = {'boxes': [[x1, y1, x2, y2], ...], 'labels': [1, ...]}\n        return annotations\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transforms:\n            image = self.transforms(image)\n        annotation = self.annotations[os.path.basename(img_path)]\n        boxes = torch.tensor(annotation['boxes'], dtype=torch.float32)\n        labels = torch.tensor(annotation['labels'], dtype=torch.int64)\n        target = {'boxes': boxes, 'labels': labels}\n        return image, target\n\n# Example usage:\ntransform = ToTensor()\ndataset = CustomDataset(root_dir='/kaggle/input/cats-and-dogs-image-classification/train/dogs', transforms=transform)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:50:43.004261Z","iopub.execute_input":"2024-07-01T10:50:43.004813Z","iopub.status.idle":"2024-07-01T10:50:43.162792Z","shell.execute_reply.started":"2024-07-01T10:50:43.004774Z","shell.execute_reply":"2024-07-01T10:50:43.161361Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Use the DataLoader to load your dataset in batches**","metadata":{}},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:51:22.219305Z","iopub.execute_input":"2024-07-01T10:51:22.219839Z","iopub.status.idle":"2024-07-01T10:51:22.227173Z","shell.execute_reply.started":"2024-07-01T10:51:22.219799Z","shell.execute_reply":"2024-07-01T10:51:22.225550Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import json\n\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, annotation_file, transforms=None):\n        self.root_dir = root_dir\n        self.transforms = transforms\n        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir)]\n        self.annotations = self.load_annotations(annotation_file)\n\n    def load_annotations(self, annotation_file):\n        with open(annotation_file) as f:\n            annotations = json.load(f)\n        return annotations\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transforms:\n            image = self.transforms(image)\n        filename = os.path.basename(img_path)\n        if filename in self.annotations:\n            annotation = self.annotations[filename]\n            boxes = torch.tensor(annotation['boxes'], dtype=torch.float32)\n            labels = torch.tensor(annotation['labels'], dtype=torch.int64)\n            target = {'boxes': boxes, 'labels': labels}\n            return image, target\n        else:\n            # Handle missing annotations\n            print(f\"No annotation for image {filename}\")\n            return None\n\n# Example usage:\nannotation_file = '/kaggle/input/cats-and-dogs-image-classification/annotations.json'\ndataset = CustomDataset(root_dir='/kaggle/input/cats-and-dogs-image-classification/train/dogs', annotation_file=annotation_file, transforms=ToTensor())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:53:04.628237Z","iopub.execute_input":"2024-07-01T10:53:04.628772Z","iopub.status.idle":"2024-07-01T10:53:04.903096Z","shell.execute_reply.started":"2024-07-01T10:53:04.628733Z","shell.execute_reply":"2024-07-01T10:53:04.901416Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m annotation_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/cats-and-dogs-image-classification/annotations.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 37\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/cats-and-dogs-image-classification/train/dogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[24], line 8\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, root_dir, annotation_file, transforms)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root_dir)]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[24], line 11\u001b[0m, in \u001b[0;36mCustomDataset.load_annotations\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_annotations\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotation_file):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m         annotations \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m annotations\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/cats-and-dogs-image-classification/annotations.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/cats-and-dogs-image-classification/annotations.json'","output_type":"error"}]},{"cell_type":"code","source":"# Define the loss functions\nrpn_cls_criterion = nn.BCEWithLogitsLoss()\nrpn_reg_criterion = nn.MSELoss()\nrcnn_cls_criterion = nn.CrossEntropyLoss()\nrcnn_reg_criterion = nn.MSELoss()\n\n# Define the model components as previously done\nanchor_sizes = ((32, 64, 128),)\naspect_ratios = ((0.5, 1.0, 2.0),)\nrpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n\nroi_pooler = MultiScaleRoIAlign(featmap_names=[\"0\"], output_size=7, sampling_ratio=2)\n\nmobilenet = torchvision.models.mobilenet_v2(weights=\"DEFAULT\")\nbackbone = nn.Sequential(*list(mobilenet.features.children()))\nbackbone.out_channels = 1280\n\nmodel = FasterRCNN(backbone=backbone, num_classes=2, anchor_generator=rpn_anchor_generator, box_roi_pool=roi_pooler)\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Move the model to the GPU if available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Training loop\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    model.train()\n    for images, targets in dataloader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n    print(f\"Epoch: {epoch}, Loss: {losses.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:51:50.346236Z","iopub.execute_input":"2024-07-01T10:51:50.347693Z","iopub.status.idle":"2024-07-01T10:51:52.210088Z","shell.execute_reply.started":"2024-07-01T10:51:50.347641Z","shell.execute_reply":"2024-07-01T10:51:52.208457Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     32\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     33\u001b[0m         targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m     29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(image)\n\u001b[0;32m---> 30\u001b[0m annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(annotation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n","\u001b[0;31mKeyError\u001b[0m: 'dog_596.jpg'"],"ename":"KeyError","evalue":"'dog_596.jpg'","output_type":"error"}]},{"cell_type":"markdown","source":"# **Faster R-CNN in PyTorch**","metadata":{}},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights = \"DEFAULT\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:42:39.578607Z","iopub.execute_input":"2024-07-01T10:42:39.579083Z","iopub.status.idle":"2024-07-01T10:42:41.886882Z","shell.execute_reply.started":"2024-07-01T10:42:39.579034Z","shell.execute_reply":"2024-07-01T10:42:41.885861Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 129MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Define number of classes and classife input size\n\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:44:56.400530Z","iopub.execute_input":"2024-07-01T10:44:56.400964Z","iopub.status.idle":"2024-07-01T10:44:56.407109Z","shell.execute_reply.started":"2024-07-01T10:44:56.400931Z","shell.execute_reply":"2024-07-01T10:44:56.405653Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Replace the model's classifer with a one with desired number of classes","metadata":{}},{"cell_type":"code","source":"model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:44:59.878585Z","iopub.execute_input":"2024-07-01T10:44:59.879080Z","iopub.status.idle":"2024-07-01T10:44:59.887823Z","shell.execute_reply.started":"2024-07-01T10:44:59.879041Z","shell.execute_reply":"2024-07-01T10:44:59.885900Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(model.roi_heads.box_predictor)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T10:45:09.944662Z","iopub.execute_input":"2024-07-01T10:45:09.945107Z","iopub.status.idle":"2024-07-01T10:45:09.953124Z","shell.execute_reply.started":"2024-07-01T10:45:09.945071Z","shell.execute_reply":"2024-07-01T10:45:09.951264Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"FastRCNNPredictor(\n  (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}