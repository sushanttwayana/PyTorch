{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer learning using BERT\n\nAt PyBooks, the company has decided to leverage the power of the BERT model, a pre-trained transformer model, for sentiment analysis. BERT has seen remarkable performance across various NLP tasks, making it a prime candidate for this use case.\n\nYou're tasked with setting up a basic workflow using the BERT model from the transformers library for binary sentiment classification.\n\n* Load the bert-base-uncased tokenizer and model suitable for binary classification.\n* Tokenize your dataset and prepare it for the model, ensuring it returns PyTorch tensors using the return_tensors argument.\n* Setup the optimizer using model parameters.","metadata":{}},{"cell_type":"code","source":"import torch \nfrom transformers import BertTokenizer, BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:28:14.832609Z","iopub.execute_input":"2024-08-06T04:28:14.832985Z","iopub.status.idle":"2024-08-06T04:28:20.802451Z","shell.execute_reply.started":"2024-08-06T04:28:14.832951Z","shell.execute_reply":"2024-08-06T04:28:20.801158Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"texts = [\"I love this day!\",\n        \"Not cup of my tea\",\n        \"Amazing Weather!\",\n        \"Very bad movie and its story.\"]\n\nlabels = [1, 0 ,1 ,0]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:29:32.429057Z","iopub.execute_input":"2024-08-06T04:29:32.430530Z","iopub.status.idle":"2024-08-06T04:29:32.435344Z","shell.execute_reply.started":"2024-08-06T04:29:32.430487Z","shell.execute_reply":"2024-08-06T04:29:32.434103Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Tokenize your data and return PyTorch tensors\ninputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=32)\ninputs[\"labels\"] = torch.tensor(labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:29:49.281717Z","iopub.execute_input":"2024-08-06T04:29:49.282148Z","iopub.status.idle":"2024-08-06T04:29:53.199867Z","shell.execute_reply.started":"2024-08-06T04:29:49.282111Z","shell.execute_reply":"2024-08-06T04:29:53.198589Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b52a951d9a44bc97fe647b9a1fed96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d371cee97a34640aaad59fe8384c655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea1451875d0400281f63b1be2b6fcf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbced2e921946c3b2c1de2d88376841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34a0cee8b554b56b058a618ca7b7ef8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Setup the optimizer using model parameters\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\nmodel.train()\nfor epoch in range(2):\n    outputs = model(**inputs)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:29:57.233160Z","iopub.execute_input":"2024-08-06T04:29:57.233924Z","iopub.status.idle":"2024-08-06T04:30:00.712977Z","shell.execute_reply.started":"2024-08-06T04:29:57.233883Z","shell.execute_reply":"2024-08-06T04:30:00.711437Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 0.7376195788383484\nEpoch: 2, Loss: 0.6524277925491333\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You've successfully fine-tuned the BERT model for sentiment analysis. This foundation will serve as a robust base for understanding user sentiments in book reviews. The output should show you model loss!","metadata":{}},{"cell_type":"markdown","source":"# Evaluating the BERT model\n\nHaving tokenized the sample reviews using BERT's tokenizer, it's now time to evaluate the BERT model with the samples at PyBooks. Additionally, you will evaluate the model's sentiment prediction on new data.\n\n* Prepare the evaluation text for the model by tokenizing it and returning PyTorch tensors.\n* Convert the output logits to probabilities between zero and one.\n* Display the sentiments from the probabilities.","metadata":{}},{"cell_type":"code","source":"text = \"I love the day very much\"\n\n# Tokenize the text and return PyTorch tensors\ninput_eval = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=32)\noutputs_eval = model(**input_eval)\n\n# Convert the output logits to probabilities\npredictions = torch.nn.functional.softmax(outputs_eval.logits, dim=-1)\n\n# Display the sentiments\npredicted_label = 'positive' if torch.argmax(predictions) > 0 else \"negative\"\nprint(f\"Text: {text}\\nSentiment: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:34:43.506113Z","iopub.execute_input":"2024-08-06T04:34:43.506530Z","iopub.status.idle":"2024-08-06T04:34:43.574489Z","shell.execute_reply.started":"2024-08-06T04:34:43.506497Z","shell.execute_reply":"2024-08-06T04:34:43.573274Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Text: I love the day very much\nSentiment: positive\n","output_type":"stream"}]},{"cell_type":"markdown","source":" Using the fine-tuned BERT model, you've accurately predicted sentiments of new texts. The printed sentiment should give you a glimpse of how the model perceives the provided text. With more training data and epochs the prediction can be improved. Remember, at PyBooks, understanding the sentiment of a review can be the key to the next bestseller recommendation!","metadata":{}}]}