{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.probability import FreqDist\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T17:24:47.810443Z","iopub.execute_input":"2024-07-16T17:24:47.810858Z","iopub.status.idle":"2024-07-16T17:24:53.048634Z","shell.execute_reply.started":"2024-07-16T17:24:47.810826Z","shell.execute_reply":"2024-07-16T17:24:53.046922Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Embedding in PyTorch\n\nPyBooks found success with a book recommendation system. However, it doesn't account for some of the semantics found in the text. PyTorch's built-in embedding layer can learn and represent the relationship between words directly from data. Your team is curious to explore this capability to improve the book recommendation system. Can you help implement it?\n\n* Map a unique index to each word in words, saving to word_to_idx.\n* Convert word_to_idx into a PyTorch tensor and save to inputs.\n* Initialize an embedding layer using the torch module with ten dimensions.\n* Pass the inputs tensor to the embedding layer and review the output.","metadata":{}},{"cell_type":"code","source":"# Map a unique index to each word\nwords = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\nword_to_idx = {word: i for i, word in enumerate(words)}\n\n# Convert word_to_idx to a tensor\ninputs =torch.LongTensor([word_to_idx[w] for w in words])\n\n# Initialize embedding layer with ten dimensions\nembedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n\n# Pass the tensor to the embedding layer\noutput = embedding(inputs)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T17:19:30.092649Z","iopub.execute_input":"2024-07-16T17:19:30.093216Z","iopub.status.idle":"2024-07-16T17:19:30.218475Z","shell.execute_reply.started":"2024-07-16T17:19:30.093181Z","shell.execute_reply":"2024-07-16T17:19:30.217166Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"tensor([[-0.6499, -0.9740, -0.0592,  0.9311,  0.5762,  0.0347,  1.6754, -0.1187,\n         -1.9626,  2.1406],\n        [-0.8724,  0.3903,  0.1551, -0.8487,  2.4329, -1.1327, -1.2070, -0.7359,\n          1.4048, -0.2865],\n        [-1.2391,  0.2109, -0.9959, -1.3934, -0.2090, -1.2180,  0.9753,  1.4358,\n         -0.8504, -0.3992],\n        [ 1.3043, -0.2193,  0.4541,  1.2761, -1.2167, -0.3780, -0.8096,  1.9629,\n         -0.4089, -0.6760],\n        [ 0.6096, -1.9553, -0.7907,  0.9744, -0.0233, -1.0991, -0.7386, -0.4441,\n         -1.3910,  0.9086],\n        [-0.5074,  0.6906,  0.1164,  0.1905,  0.7469, -0.7672,  0.2377,  0.4494,\n         -0.8641,  0.8797],\n        [ 0.7060,  0.0937,  0.1582,  0.1275, -1.2789,  2.4429, -0.5852, -0.3099,\n          0.7133, -0.2859],\n        [ 0.1998,  0.0303, -1.4978,  0.5402,  1.9271,  1.0076,  0.3538, -0.7339,\n         -0.1393, -0.2588],\n        [ 1.7900,  0.1728,  0.2610,  0.1896,  0.7696,  0.3569,  1.9528,  2.0910,\n          1.1782,  0.4494],\n        [-0.3011, -1.4055,  0.3519,  1.3781, -0.4644,  1.6756,  0.5829,  1.5813,\n         -0.6373,  0.2445],\n        [-0.8843,  1.0977,  0.3833, -1.1744,  0.8947, -0.1450,  0.9080,  1.1931,\n         -1.2793, -0.4740],\n        [-1.5824,  0.6733, -1.3561,  0.1363,  1.8281, -0.7883, -0.1004, -0.1221,\n          0.4983, -0.1433],\n        [-1.2391,  0.2109, -0.9959, -1.3934, -0.2090, -1.2180,  0.9753,  1.4358,\n         -0.8504, -0.3992],\n        [-1.8144, -0.8980, -0.0622,  0.0595, -1.8907, -1.6248, -0.8056,  0.5494,\n          1.7286,  0.7017],\n        [ 0.7221, -0.3066,  0.0260,  1.7550,  0.4945,  0.3567, -0.9452,  0.3990,\n         -0.2306, -1.0990]], grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We've just implemented PyTorch's nn.Embedding for PyBooks. The output shows each elements representing the respective word embeddings. Keep up the great work!","metadata":{}},{"cell_type":"markdown","source":"**Using embeddings in the pipeline**\n\nHere is what this step would look like in the full pipeline with the dataset and dataloader we previously created. Here, we perform our embedding on the data generated by the dataloader.\n","metadata":{}},{"cell_type":"code","source":"# Create a list of stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n# Initialize the tokenizer and stemmer\ntokenizer = get_tokenizer(\"basic_english\")\nstemmer = PorterStemmer() \n\n# Complete the function to preprocess sentences\ndef preprocess_sentences(sentences):\n    processed_sentences = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        tokens = tokenizer(sentence)\n        tokens = [token for token in tokens if token not in stop_words]\n        tokens = [stemmer.stem(token) for token in tokens]\n        processed_sentences.append(' '.join(tokens))\n    return processed_sentences","metadata":{"execution":{"iopub.status.busy":"2024-07-16T17:25:19.204502Z","iopub.execute_input":"2024-07-16T17:25:19.205147Z","iopub.status.idle":"2024-07-16T17:25:19.215820Z","shell.execute_reply.started":"2024-07-16T17:25:19.205073Z","shell.execute_reply":"2024-07-16T17:25:19.214037Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create a CLass\nclass TextDataset(Dataset):\n    def __init__(self, encoded_sentences):\n        self.data = encoded_sentences\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-16T17:27:10.742672Z","iopub.execute_input":"2024-07-16T17:27:10.743161Z","iopub.status.idle":"2024-07-16T17:27:10.752303Z","shell.execute_reply.started":"2024-07-16T17:27:10.743123Z","shell.execute_reply":"2024-07-16T17:27:10.749885Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def text_processing_pipeline(text):\n    tokens = preprocess_sentences(text)\n    dataset = TextDataset(tokens)\n    dataloader = DataLoader(dataset, batch_size =2, shuffle = True)\n    \n    return dataloader, vectorizer","metadata":{"execution":{"iopub.status.busy":"2024-07-16T17:31:22.274825Z","iopub.execute_input":"2024-07-16T17:31:22.276290Z","iopub.status.idle":"2024-07-16T17:31:22.282990Z","shell.execute_reply.started":"2024-07-16T17:31:22.276232Z","shell.execute_reply":"2024-07-16T17:31:22.281600Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"text = \"Your sample text here\"\n\ndataloader, vectorizer = text_processing_pipeline(text)\nembedding = nn.Embedding(num_embedding = 10, embedding_dim = 50)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T17:31:24.459863Z","iopub.execute_input":"2024-07-16T17:31:24.460939Z","iopub.status.idle":"2024-07-16T17:31:24.805660Z","shell.execute_reply.started":"2024-07-16T17:31:24.460890Z","shell.execute_reply":"2024-07-16T17:31:24.804169Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour sample text here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dataloader, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mtext_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(num_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mtext_processing_pipeline\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextDataset(tokens)\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataloader, \u001b[43mvectorizer\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"],"ename":"NameError","evalue":"name 'vectorizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"for batch in dataloader :\n    output = embedding(batch)\n    print(output)","metadata":{},"execution_count":null,"outputs":[]}]}