{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text completion with pre-trained GPT-2 models\n\nBack at PyBooks, your current project involves creating captivating narratives based on existing stories to engage customers and enhance their experience. To achieve this, you need a powerful text generation tool that can seamlessly generate compelling text continuations. You'll be using a pre-trained model to get the job done.\n\n\n* Initialize the pre-trained GPT-2 tokenizer for gpt2.\n* Initialize the pre-trained GPT-2 LMHead model for gpt2.\n* Encode the seed text to get input tensors using seed_text.\n* Generate the text by defining the parameters for the input considering a max_length of 100, temperature of 0.7, and an no_repeat_ngram_size of 2 in the GPT-2 model.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:32:42.378248Z","iopub.execute_input":"2024-08-05T10:32:42.378687Z","iopub.status.idle":"2024-08-05T10:32:46.599628Z","shell.execute_reply.started":"2024-08-05T10:32:42.378652Z","shell.execute_reply":"2024-08-05T10:32:46.598627Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Initialize the pre-trained model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\nseed_text = \"Once upon a time\"\n\n# Encode the seed text to get input tensors\ninput_ids = tokenizer.encode(seed_text, return_tensors='pt')\n\n# Generate text from the model\noutput = model.generate(input_ids, max_length=100, temperature=0.7, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id) \n\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T10:32:49.212018Z","iopub.execute_input":"2024-08-05T10:32:49.212808Z","iopub.status.idle":"2024-08-05T10:32:57.931035Z","shell.execute_reply.started":"2024-08-05T10:32:49.212774Z","shell.execute_reply":"2024-08-05T10:32:57.929902Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36fde5a885634dcca982af184aa9e130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38bfd38654984f2682bc0f99101165f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131234d7ef13461fbd11f0579d4edd85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe2ed22f53c47acbaa8865cff9c8a99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e377bee43f4d4f40a7fcc784fae94f60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"628902c869fd47898a585ee6f47bfe95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46c817b0b83540409d3952f85702dd5b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n\nThe world that was created was not the same as the one that is now. It was an endless, endless world. And the Gods were not born of nothing. They were created of a single, single thing. That was why the universe was so beautiful. Because the cosmos was made of two\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You have successfully completed the text completion exercise using the GPT-2 model and tokenizer. By analyzing the generated output, you can explore the fascinating world of language generation and witness the model's ability to generate imaginative and coherent text based on the provided seed text. Well done!","metadata":{}},{"cell_type":"markdown","source":"# Language translation with pretrained PyTorch model\n\nYour team at PyBooks is working on an AI project that involves translation from one language to another. They want to leverage pre-trained models for this task, which can save a lot of training time and resources. The task for this exercise is to set up a translation model from HuggingFace's Transformers library, specifically the T5 (Text-To-Text Transfer Transformer) model, and use it to translate an English phrase to French.\n\nT5Tokenizer, T5ForConditionalGeneration \nInitialize the tokenizer and model from the pretrained \"t5-small\" model.\nEncode the input prompt using the tokenizer, making sure to return PyTorch tensors.\nTranslate the input prompt using model and generate the translated output.","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Initalize tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\ninput_prompt = \"translate English to French: 'Hello, how are you?'\"\n\n# Encode the input prompt using the tokenizer\ninput_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n\n# Generate the translated ouput\noutput = model.generate(input_ids, max_length=50)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated text:\",generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-05T12:07:58.792446Z","iopub.execute_input":"2024-08-05T12:07:58.792856Z","iopub.status.idle":"2024-08-05T12:08:15.220003Z","shell.execute_reply.started":"2024-08-05T12:07:58.792823Z","shell.execute_reply":"2024-08-05T12:08:15.218670Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c9d8498978420f967a5d77e8170761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6cf0051c134fae887b05c379f14d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e773871a71d54839840552a2790e019a"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d5ad6ad8b14f589fc8af4a574f1781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4e397dec2d46159cb7148ffcb100c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f238bf114e414c848b0ea0b1fa42bc16"}},"metadata":{}},{"name":"stdout","text":"Generated text: \"Jo, comment Ãªtes-vous?\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":" You've successfully used a pre-trained T5 model to translate an English phrase to French. This example shows the power of transformer models and how they can be leveraged for various NLP tasks, including translation. Well done!","metadata":{}}]}