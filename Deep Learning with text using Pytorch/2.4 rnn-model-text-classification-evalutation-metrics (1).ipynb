{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:42:03.522344Z","iopub.execute_input":"2024-08-04T15:42:03.522920Z","iopub.status.idle":"2024-08-04T15:42:04.222894Z","shell.execute_reply.started":"2024-08-04T15:42:03.522886Z","shell.execute_reply":"2024-08-04T15:42:04.221781Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load dataset\ncategories = ['rec.autos', 'sci.med', 'comp.graphics']\nnewsgroups = fetch_20newsgroups(subset='all', categories=categories)\nvectorizer = CountVectorizer(max_features=5000)\nX = vectorizer.fit_transform(newsgroups.data).toarray()\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(newsgroups.target)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:42:06.767061Z","iopub.execute_input":"2024-08-04T15:42:06.767443Z","iopub.status.idle":"2024-08-04T15:42:18.339326Z","shell.execute_reply.started":"2024-08-04T15:42:06.767393Z","shell.execute_reply":"2024-08-04T15:42:18.338245Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(y[:10])","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:42:31.155725Z","iopub.execute_input":"2024-08-04T15:42:31.156131Z","iopub.status.idle":"2024-08-04T15:42:31.162008Z","shell.execute_reply.started":"2024-08-04T15:42:31.156102Z","shell.execute_reply":"2024-08-04T15:42:31.160872Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[1 0 1 1 0 0 2 0 2 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_seq = torch.tensor(X_train, dtype=torch.float32)\nX_test_seq = torch.tensor(X_test, dtype=torch.float32)\ny_train_seq = torch.tensor(y_train, dtype=torch.long)\ny_test_seq = torch.tensor(y_test, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:42:33.742937Z","iopub.execute_input":"2024-08-04T15:42:33.743332Z","iopub.status.idle":"2024-08-04T15:42:33.852919Z","shell.execute_reply.started":"2024-08-04T15:42:33.743301Z","shell.execute_reply":"2024-08-04T15:42:33.851901Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Parameters\ninput_size = X_train_seq.shape[1]\nhidden_size = 32\nnum_layers = 2\nnum_classes = 3","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:42:35.820077Z","iopub.execute_input":"2024-08-04T15:42:35.820488Z","iopub.status.idle":"2024-08-04T15:42:35.825347Z","shell.execute_reply.started":"2024-08-04T15:42:35.820456Z","shell.execute_reply":"2024-08-04T15:42:35.824223Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Building an RNN model for text\n\nAs a data analyst at PyBooks, you often encounter datasets that contain sequential information, such as customer interactions, time series data, or text documents. RNNs can effectively analyze and extract insights from such data. In this exercise, you will dive into the Newsgroup dataset that has already been processed and encoded for you. This dataset comprises articles from different categories. Your task is to apply an RNN to classify these articles into three categories:\n\nrec.autos, sci.med, and comp.graphics.\n\nThis and the following exercises use the fetch_20newsgroups dataset from sklearn.\n\n* Complete the RNN class with an RNN layer and a fully connected linear layer.\n* Initialize the model.\n* Train the RNN model for ten epochs by zeroing the gradients.","metadata":{}},{"cell_type":"code","source":"class RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)        \n        \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.rnn(x.unsqueeze(1), h0)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out\n\n# Initialize the model\nrnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(rnn_model.parameters(), lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:48:07.372376Z","iopub.execute_input":"2024-08-04T15:48:07.372785Z","iopub.status.idle":"2024-08-04T15:48:07.385701Z","shell.execute_reply.started":"2024-08-04T15:48:07.372749Z","shell.execute_reply":"2024-08-04T15:48:07.384524Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Train the model for 50 epochs\nfor epoch in range(50):\n    rnn_model.train()\n    optimizer.zero_grad()\n    outputs = rnn_model(X_train_seq)\n    loss = criterion(outputs, y_train_seq)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:48:20.095497Z","iopub.execute_input":"2024-08-04T15:48:20.095933Z","iopub.status.idle":"2024-08-04T15:48:21.246471Z","shell.execute_reply.started":"2024-08-04T15:48:20.095903Z","shell.execute_reply":"2024-08-04T15:48:21.245378Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 1.1557263135910034\nEpoch: 2, Loss: 1.0302963256835938\nEpoch: 3, Loss: 0.9026435017585754\nEpoch: 4, Loss: 0.7266136407852173\nEpoch: 5, Loss: 0.548943281173706\nEpoch: 6, Loss: 0.39736050367355347\nEpoch: 7, Loss: 0.27267393469810486\nEpoch: 8, Loss: 0.18637728691101074\nEpoch: 9, Loss: 0.12769444286823273\nEpoch: 10, Loss: 0.08587413281202316\nEpoch: 11, Loss: 0.05711580440402031\nEpoch: 12, Loss: 0.038773197680711746\nEpoch: 13, Loss: 0.027011841535568237\nEpoch: 14, Loss: 0.019469747319817543\nEpoch: 15, Loss: 0.014546065591275692\nEpoch: 16, Loss: 0.011154643259942532\nEpoch: 17, Loss: 0.008809257298707962\nEpoch: 18, Loss: 0.007102936506271362\nEpoch: 19, Loss: 0.005821355618536472\nEpoch: 20, Loss: 0.004847390111535788\nEpoch: 21, Loss: 0.004096565302461386\nEpoch: 22, Loss: 0.0034660627134144306\nEpoch: 23, Loss: 0.0028982353396713734\nEpoch: 24, Loss: 0.00237806374207139\nEpoch: 25, Loss: 0.0019221705151721835\nEpoch: 26, Loss: 0.001561245764605701\nEpoch: 27, Loss: 0.0013058981858193874\nEpoch: 28, Loss: 0.001133846933953464\nEpoch: 29, Loss: 0.001015047891996801\nEpoch: 30, Loss: 0.0009272939641959965\nEpoch: 31, Loss: 0.0008585855830460787\nEpoch: 32, Loss: 0.0008023151312954724\nEpoch: 33, Loss: 0.0007545673288404942\nEpoch: 34, Loss: 0.0007127920398488641\nEpoch: 35, Loss: 0.000675260613206774\nEpoch: 36, Loss: 0.0006408660556189716\nEpoch: 37, Loss: 0.0006089844973757863\nEpoch: 38, Loss: 0.0005793426535092294\nEpoch: 39, Loss: 0.0005518830148503184\nEpoch: 40, Loss: 0.0005266243242658675\nEpoch: 41, Loss: 0.0005035802023485303\nEpoch: 42, Loss: 0.0004827021330129355\nEpoch: 43, Loss: 0.00046388438204303384\nEpoch: 44, Loss: 0.0004469705745577812\nEpoch: 45, Loss: 0.0004317792772781104\nEpoch: 46, Loss: 0.0004181293770670891\nEpoch: 47, Loss: 0.00040583848021924496\nEpoch: 48, Loss: 0.00039474121876992285\nEpoch: 49, Loss: 0.0003846908512059599\nEpoch: 50, Loss: 0.0003755591169465333\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Model loss should always decrease as it shows how well the model has learned new patterns. Keep up the excellent work!","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\nrnn_model.eval()\nwith torch.no_grad():\n    outputs = rnn_model(X_test_seq)\n    _, predicted = torch.max(outputs, 1)\n    accuracy = accuracy_score(y_test_seq, predicted)\n    print(f'Test Accuracy: {accuracy:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:48:34.914224Z","iopub.execute_input":"2024-08-04T15:48:34.914639Z","iopub.status.idle":"2024-08-04T15:48:34.926258Z","shell.execute_reply.started":"2024-08-04T15:48:34.914606Z","shell.execute_reply":"2024-08-04T15:48:34.925099Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.97\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building an LSTM model for text\n\nAt PyBooks, the team is constantly seeking to enhance the user experience by leveraging the latest advancements in technology. In line with this vision, they have assigned you a critical task. The team wants you to explore the potential of another powerful tool: LSTM, known for capturing more complexities in data patterns. You are working with the same Newsgroup dataset, with the objective remaining unchanged: to classify news articles into three distinct categories:\n\nrec.autos, sci.med, and comp.graphics.\n\n* Set up an LSTM model by completing the LSTM and linear layers with the necessary parameters.\n* Initialize the model with the necessary parameters.\n* Train the LSTM model resetting the gradients to zero and passing the input data X_train_seq through the model.\n* Calculate the loss based on the predicted outputs and the true labels.\n\n\n","metadata":{}},{"cell_type":"code","source":"class LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)        \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))  # Reshape input to (batch_size, seq_length, input_size)\n        out = out[:, -1, :] \n        out = self.fc(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:43:47.417523Z","iopub.execute_input":"2024-08-04T15:43:47.418276Z","iopub.status.idle":"2024-08-04T15:43:47.426116Z","shell.execute_reply.started":"2024-08-04T15:43:47.418242Z","shell.execute_reply":"2024-08-04T15:43:47.424843Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Initialize model with required parameters\nlstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n\n# Train the model by passing the correct parameters and zeroing the gradient\nfor epoch in range(10): \n    optimizer.zero_grad()\n    outputs = lstm_model(X_train_seq)\n    loss = criterion(outputs, y_train_seq)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:14:32.514815Z","iopub.execute_input":"2024-08-04T16:14:32.515488Z","iopub.status.idle":"2024-08-04T16:14:34.033629Z","shell.execute_reply.started":"2024-08-04T16:14:32.515454Z","shell.execute_reply":"2024-08-04T16:14:34.032478Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 1.1131354570388794\nEpoch: 2, Loss: 1.0890493392944336\nEpoch: 3, Loss: 1.054835557937622\nEpoch: 4, Loss: 1.0028889179229736\nEpoch: 5, Loss: 0.9312984943389893\nEpoch: 6, Loss: 0.8425112962722778\nEpoch: 7, Loss: 0.7449913024902344\nEpoch: 8, Loss: 0.6447567343711853\nEpoch: 9, Loss: 0.5466322302818298\nEpoch: 10, Loss: 0.4525633752346039\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model\nlstm_model.eval()\nwith torch.no_grad():\n    outputs = lstm_model(X_test_seq)\n    _, y_pred_lstm = torch.max(outputs, 1)\n#     accuracy = accuracy_score(y_test_seq, y_pred_lstm)\n#     print(f'Test Accuracy: {accuracy:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:15:09.073357Z","iopub.execute_input":"2024-08-04T16:15:09.073800Z","iopub.status.idle":"2024-08-04T16:15:09.108540Z","shell.execute_reply.started":"2024-08-04T16:15:09.073766Z","shell.execute_reply":"2024-08-04T16:15:09.107481Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":" The output presents model loss that would keep decreasing with each epoch. This information could be utilized by the team at PyBooks to compare with other models. Keep up the great w","metadata":{}},{"cell_type":"markdown","source":"# Building a GRU model for text\n\nAt PyBooks, the team has been impressed with the performance of the two models you previously trained. However, in their pursuit of excellence, they want to ensure the selection of the absolute best model for the task at hand. Therefore, they have asked you to further expand the project by experimenting with the capabilities of GRU models, renowned for their efficiency and effectiveness in text classification tasks. Your new assignment is to apply the GRU model to classify articles from the Newsgroup dataset into the following categories:\n\nrec.autos, sci.med, and comp.graphics.\n\n* Complete the GRU class with the required parameters.\n* Initialize the model with the same parameters.\n* Train the model: pass the parameters to the criterion function, and backpropagate the loss.","metadata":{}},{"cell_type":"code","source":"# Complete the GRU model\nclass GRUModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(GRUModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n        \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n        out, _ = self.gru(x.unsqueeze(1), h0)  # Reshape input to (batch_size, seq_length, input_size)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:44:02.275895Z","iopub.execute_input":"2024-08-04T15:44:02.276287Z","iopub.status.idle":"2024-08-04T15:44:02.287560Z","shell.execute_reply.started":"2024-08-04T15:44:02.276256Z","shell.execute_reply":"2024-08-04T15:44:02.286102Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\ngru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n\n# Train the model and backpropagate the loss after initialization\nfor epoch in range(15):\n    optimizer.zero_grad()\n    outputs = gru_model(X_train_seq)\n    loss = criterion(outputs, y_train_seq)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:15:14.519191Z","iopub.execute_input":"2024-08-04T16:15:14.519582Z","iopub.status.idle":"2024-08-04T16:15:15.342476Z","shell.execute_reply.started":"2024-08-04T16:15:14.519553Z","shell.execute_reply":"2024-08-04T16:15:15.341247Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 1.0958889722824097\nEpoch: 2, Loss: 1.0295681953430176\nEpoch: 3, Loss: 0.9211702942848206\nEpoch: 4, Loss: 0.7771641612052917\nEpoch: 5, Loss: 0.6244705319404602\nEpoch: 6, Loss: 0.475839227437973\nEpoch: 7, Loss: 0.34952712059020996\nEpoch: 8, Loss: 0.24499188363552094\nEpoch: 9, Loss: 0.16498719155788422\nEpoch: 10, Loss: 0.10784843564033508\nEpoch: 11, Loss: 0.06953068822622299\nEpoch: 12, Loss: 0.04533623531460762\nEpoch: 13, Loss: 0.030256202444434166\nEpoch: 14, Loss: 0.020781321451067924\nEpoch: 15, Loss: 0.01461486890912056\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model\ngru_model.eval()\nwith torch.no_grad():\n    outputs = gru_model(X_test_seq)\n    _, y_pred_gru = torch.max(outputs, 1)\n#     accuracy = accuracy_score(y_test_seq, predicted)\n#     print(f'Test Accuracy: {accuracy:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:15:29.231914Z","iopub.execute_input":"2024-08-04T16:15:29.232303Z","iopub.status.idle":"2024-08-04T16:15:29.246264Z","shell.execute_reply.started":"2024-08-04T16:15:29.232271Z","shell.execute_reply":"2024-08-04T16:15:29.245076Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"You've effectively trained GRU models for text classification. The decreasing model loss across epochs is promising, and can be used by the PyBooks team for comparison with other models!\n","metadata":{}},{"cell_type":"markdown","source":"# Evaluating RNN classifications\n","metadata":{}},{"cell_type":"markdown","source":"The team at PyBooks now wants you to evaluate the RNN model you created and ran using the Newsgroup dataset. Recall, the goal was to classify the articles into one of three categories:\n\nrec.autos, sci.med, and comp.graphics.\n\n\nAn instance of rnn_model trained in the previous exercise in preloaded for you, too.\n\n\n* Create an instance of each metric for multi-class classification with num_classes equal to the number of categories.\n* Generate the predictions for the rnn_model using the test data X_test_seq.\n* Calculate the metrics using the predicted classes and the true labels.","metadata":{}},{"cell_type":"code","source":"from torchmetrics import Accuracy, Precision, F1Score,Recall\n\n# Create an instance of the metrics\naccuracy = Accuracy(task=\"multiclass\", num_classes = 3)\nprecision = Precision(task = \"multiclass\", num_classes=3)\nrecall = Recall(task=\"multiclass\", num_classes=3)\nf1 = F1Score(task=\"multiclass\", num_classes=3)\n\n# Generate the predictions\noutputs = rnn_model(X_test_seq)\n_, predicted = torch.max(outputs, 1)\n\n# Calculate the metrics\naccuracy_score = accuracy(predicted, y_test_seq)\nprecision_score = precision(predicted, y_test_seq)\nrecall_score = recall(predicted, y_test_seq)\nf1_score = f1(predicted, y_test_seq)\nprint(\"RNN Model - \\n Accuracy: {} \\n Precision: {} \\n Recall: {} \\n F1 Score: {} \\n\".format(accuracy_score, precision_score, recall_score, f1_score))","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:49:25.707498Z","iopub.execute_input":"2024-08-04T15:49:25.707918Z","iopub.status.idle":"2024-08-04T15:49:25.726858Z","shell.execute_reply.started":"2024-08-04T15:49:25.707884Z","shell.execute_reply":"2024-08-04T15:49:25.725440Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"RNN Model - \n Accuracy: 0.9712352156639099 \n Precision: 0.9712352156639099 \n Recall: 0.9712352156639099 \n F1 Score: 0.9712352156639099 \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The model metrics provide significant insights on the effectiveness of our model. We can notice that all metrics are around 0.97 which is a good sign. Keep up the excellent work!","metadata":{}},{"cell_type":"markdown","source":"**Evaluating the model's performance**\n\nThe PyBooks team has been making strides on the book recommendation engine. The modeling team has provided you two different models ready for your book recommendation engine at PyBooks. One model is based on LSTM (lstm_model) and the other uses a GRU (gru_model). You've been tasked to evaluate and compare these models.\n\nThe testing labels y_test and the model's predictions y_pred_lstm for lstm_model and y_pred_gru for gru_model.\n\n* Define accuracy, precision, recall and F1 for multi-class classification by specifying num_classes and task.\n* Calculate and print the accuracy, precision, recall, and F1 score for lstm_model.\n* Similarly, calculate the evaluation metrics for gru_model.","metadata":{}},{"cell_type":"code","source":"# Create an instance of the metrics\naccuracy = Accuracy(task=\"multiclass\", num_classes=3)\nprecision = Precision(task=\"multiclass\", num_classes=3)\nrecall = Recall(task=\"multiclass\", num_classes=3)\nf1 = F1Score(task=\"multiclass\", num_classes=3)\n\n# Calculate metrics for the LSTM model\naccuracy_1 = accuracy(y_pred_lstm, y_test_seq)\nprecision_1 = precision(y_pred_lstm, y_test_seq)\nrecall_1 = recall(y_pred_lstm, y_test_seq)\nf1_1 = f1(y_pred_lstm, y_test_seq)\nprint(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n\n# Calculate metrics for the GRU model\naccuracy_2 = accuracy(y_pred_gru, y_test_seq)\nprecision_2 = precision(y_pred_gru, y_test_seq)\nrecall_2 = recall(y_pred_gru, y_test_seq)\nf1_2 = f1(y_pred_gru, y_test_seq)\nprint(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))","metadata":{"execution":{"iopub.status.busy":"2024-08-04T16:16:29.514266Z","iopub.execute_input":"2024-08-04T16:16:29.515046Z","iopub.status.idle":"2024-08-04T16:16:29.533887Z","shell.execute_reply.started":"2024-08-04T16:16:29.515010Z","shell.execute_reply":"2024-08-04T16:16:29.532723Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"LSTM Model - Accuracy: 0.9492385983467102, Precision: 0.9492385983467102, Recall: 0.9492385983467102, F1 Score: 0.9492385983467102\nGRU Model - Accuracy: 0.9644669890403748, Precision: 0.9644669890403748, Recall: 0.9644669890403748, F1 Score: 0.9644669890403748\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Well done! You've evaluated and compared two different models. Now, PyBooks can decide which model to deploy for their book recommendation engine. ","metadata":{}}]}