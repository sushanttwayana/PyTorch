{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.probability import FreqDist\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T14:58:05.544773Z","iopub.execute_input":"2024-07-13T14:58:05.545210Z","iopub.status.idle":"2024-07-13T14:58:05.552415Z","shell.execute_reply.started":"2024-07-13T14:58:05.545175Z","shell.execute_reply":"2024-07-13T14:58:05.551128Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# The Universal Pipeline for the Text Processing","metadata":{}},{"cell_type":"markdown","source":". We create a custom class, TextDataset, serving as our data container. The init method initializes the dataset with the input text data. The len method returns the total number of samples in the dataset, and the getitem method allows us to access a specific sample at a given index. This class, extending PyTorch's Dataset, allows us to organize and access our text data efficiently.","metadata":{}},{"cell_type":"code","source":"# Create a CLass\nclass TextDataset(Dataset):\n    def __init__(self, text):\n        self.text = text\n    def __len__(self):\n        return len(self.text)\n    def __getitem__(self, idx):\n        return self.text[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:32:12.698039Z","iopub.execute_input":"2024-07-13T15:32:12.698479Z","iopub.status.idle":"2024-07-13T15:32:12.706023Z","shell.execute_reply.started":"2024-07-13T15:32:12.698442Z","shell.execute_reply":"2024-07-13T15:32:12.704549Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"**Using helper functions**\n\nFor convenience, we'll use helper functions for preprocessing and encoding. preprocess_sentences combines the techniques we've covered; we can also customize it to only include specific techniques depending on the problem. We've chosen CountVectorizer in encode_sentences to convert the cleaned sentences into arrays. We've included an extract_sentences function that uses regular expressions (regex) to convert English sentences. While regex is beyond the scope of this course, we've included it here for potential use in the pre-exercise code.\n","metadata":{}},{"cell_type":"code","source":"# Create a list of stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n# Initialize the tokenizer and stemmer\ntokenizer = get_tokenizer(\"basic_english\")\nstemmer = PorterStemmer() \n\ndef preprocess_sentences(sentences):\n    processed_sentences = []\n    \n    for sentence in sentences:\n        sentence = sentence.lower()\n        tokens = tokenizer(sentence)\n        tokens = [token for token in tokens if token not in stop_words]\n        tokens = [stemmer.stem(token) for token in tokens]\n        \n        freq_dist = FreqDist(tokens)\n        threshold = 1\n        tokens = [token for token in tokens if freq_dist[token] > threshold]\n        processed_sentences.append(' '.join(tokens))\n        \n    print(\"Processed sentences:\", processed_sentences)  # Debug print\n    return processed_sentences","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:27:28.848606Z","iopub.execute_input":"2024-07-13T15:27:28.849052Z","iopub.status.idle":"2024-07-13T15:27:28.859117Z","shell.execute_reply.started":"2024-07-13T15:27:28.849019Z","shell.execute_reply":"2024-07-13T15:27:28.857675Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def preprocess_sentences(sentences):\n    processed_sentences = []\n    \n    for sentence in sentences:\n        sentence = sentence.lower()\n        tokens = tokenizer(sentence)\n        tokens = [token for token in tokens if token not in stop_words and token.isalpha()]  # Ensure only alphabetic tokens are retained\n        tokens = [stemmer.stem(token) for token in tokens]\n        \n        # Remove frequency threshold to retain as many tokens as possible\n        processed_sentences.append(' '.join(tokens))\n        \n    print(\"Processed sentences:\", processed_sentences)  # Debug print\n    return processed_sentences","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:31:15.964937Z","iopub.execute_input":"2024-07-13T15:31:15.965393Z","iopub.status.idle":"2024-07-13T15:31:15.973195Z","shell.execute_reply.started":"2024-07-13T15:31:15.965357Z","shell.execute_reply":"2024-07-13T15:31:15.971963Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def encode_sentences(sentences):\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(sentences)\n    encoded_sentences = X.toarray()\n    return encoded_sentences, vectorizer","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:31:53.164821Z","iopub.execute_input":"2024-07-13T15:31:53.165241Z","iopub.status.idle":"2024-07-13T15:31:53.171283Z","shell.execute_reply.started":"2024-07-13T15:31:53.165210Z","shell.execute_reply":"2024-07-13T15:31:53.170162Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def extract_sentences(data):\n    sentences = re.findall(r'[A-Z][^.!?]*[.!?]',data)\n    \n    return sentences","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:31:55.099958Z","iopub.execute_input":"2024-07-13T15:31:55.100396Z","iopub.status.idle":"2024-07-13T15:31:55.106411Z","shell.execute_reply.started":"2024-07-13T15:31:55.100361Z","shell.execute_reply":"2024-07-13T15:31:55.105060Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"**Constructing the text processing pipeline**\n\nNow, let's construct our text processing pipeline. We define a function text_processing_pipeline that takes raw text as input. Within this function, we preprocess the text using the preprocess_sentences function. This returns a list of tokens. Next, we convert these tokens into numerical vectors using the encode_sentences function. After encoding, we instantiate our PyTorch TextDataset with the numerical vectors, then initialize a DataLoader with this dataset. The DataLoader will allow us to iterate over the dataset in manageable batches of size two and in a shuffled manner, ensuring a diverse mix of examples in each batch.\n","metadata":{}},{"cell_type":"code","source":"def text_processing_pipeline(text):\n    tokens = preprocess_sentences(text)\n    encoded_sentences, vectorizer = encode_sentences(tokens)\n    dataset = TextDataset(encoded_sentences)\n    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n    return dataloader, vectorizer\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:31:57.138075Z","iopub.execute_input":"2024-07-13T15:31:57.138509Z","iopub.status.idle":"2024-07-13T15:31:57.144995Z","shell.execute_reply.started":"2024-07-13T15:31:57.138472Z","shell.execute_reply":"2024-07-13T15:31:57.143714Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Applying the text processing pipeline**\n\nWith our text processing pipeline function ready, we can apply it to any text data. Let's say we have two sentences: \"This is the first text data\" and \"And here is another one\". We call the extract sentences function to convert the text to sentences. We feed each of these sentences into our text_processing_pipeline function. This preprocesses, encodes, and loads them into individual DataLoaders, stored in the dataloaders list using list comprehension. We also store an instance of the vectorizer created during encoding to access the feature names for each vector. Finally, the print statement uses the next iter combination and allows us to access the batches of data from the dataloaders. The output is the first ten components of the first batch in the dataloader. It contains the encoded representation of the sentences that represent the frequency of the first five words in the vocabulary for each sentence.\n","metadata":{}},{"cell_type":"code","source":"# Applying the text processing pipeline\ntext_data = \"This is the first text data. And here is another one.\"\nsentences = extract_sentences(text_data)\n\ndataloaders = []\nvectorizers = []\n\nfor text in sentences:\n    dataloader, vectorizer = text_processing_pipeline([text])  # Pass as list of one sentence\n    dataloaders.append(dataloader)\n    vectorizers.append(vectorizer)\n\nfor vectorizer in vectorizers:\n    print(\"Feature names:\", vectorizer.get_feature_names_out()) \n\nfor dataloader in dataloaders:\n    print(\"Next batch:\", next(iter(dataloader)))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:32:17.252922Z","iopub.execute_input":"2024-07-13T15:32:17.253363Z","iopub.status.idle":"2024-07-13T15:32:17.268203Z","shell.execute_reply.started":"2024-07-13T15:32:17.253321Z","shell.execute_reply":"2024-07-13T15:32:17.266919Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Processed sentences: ['first text data']\nProcessed sentences: ['anoth one']\nFeature names: ['data' 'first' 'text']\nFeature names: ['anoth' 'one']\nNext batch: tensor([[1, 1, 1]])\nNext batch: tensor([[1, 1]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1. Processed Sentences: The sentences \"first text data\" and \"another one\" have been correctly processed by tokenization, stop word removal, stemming, and encoding using CountVectorizer. Each sentence is represented by the most frequent tokens found in it.\n\n2. Feature Names: The vectorizer.get_feature_names_out() correctly displays the unique tokens (features) identified in each sentence after preprocessing and encoding.\n\n3. Next Batch: The tensors printed represent the encoded representations of each sentence. For example, the first batch tensor tensor([[1, 1, 1]]) indicates that in the first batch, both sentences \"first text data\" and \"another one\" are represented with the counts [1, 1, 1] for the features 'data', 'first', and 'text' respectively.\n\nTherefore, based on the provided output, your text processing pipeline is indeed correct and functioning as expected. It preprocesses the text, encodes it using CountVectorizer, and prepares it for use in machine learning or natural language processing tasks.","metadata":{}},{"cell_type":"code","source":"text_data = \"This is the first text data. And here is the another one.\"\nsentences = extract_sentences(text_data)\n# sentences = text_data.split('.')  # Simple sentence extraction\ndataloaders, vectorizer = [text_processing_pipeline(text) for text in sentences]\n\nprint(vectorizer.get_feature_names_out()) \nprint(next(iter(dataloader)))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:32:24.358316Z","iopub.execute_input":"2024-07-13T15:32:24.358747Z","iopub.status.idle":"2024-07-13T15:32:24.513117Z","shell.execute_reply.started":"2024-07-13T15:32:24.358713Z","shell.execute_reply":"2024-07-13T15:32:24.511617Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Processed sentences: ['', 'h', '', '', '', '', '', '', '', 'h', 'e', '', 'f', '', 'r', '', '', '', '', 'e', 'x', '', '', '', '', '', '', '']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[58], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m extract_sentences(text_data)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sentences = text_data.split('.')  # Simple sentence extraction\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataloaders, vectorizer \u001b[38;5;241m=\u001b[39m [text_processing_pipeline(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()) \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader)))\n","Cell \u001b[0;32mIn[58], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m extract_sentences(text_data)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sentences = text_data.split('.')  # Simple sentence extraction\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataloaders, vectorizer \u001b[38;5;241m=\u001b[39m [\u001b[43mtext_processing_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()) \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader)))\n","Cell \u001b[0;32mIn[52], line 3\u001b[0m, in \u001b[0;36mtext_processing_pipeline\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_processing_pipeline\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m preprocess_sentences(text)\n\u001b[0;32m----> 3\u001b[0m     encoded_sentences, vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mencode_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m TextDataset(encoded_sentences)\n\u001b[1;32m      5\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[50], line 3\u001b[0m, in \u001b[0;36mencode_sentences\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_sentences\u001b[39m(sentences):\n\u001b[1;32m      2\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 3\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     encoded_sentences \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_sentences, vectorizer\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n","\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"],"ename":"ValueError","evalue":"empty vocabulary; perhaps the documents only contain stop words","output_type":"error"}]},{"cell_type":"code","source":"print(vectorizer.get_feature_names_out()) \nprint(next(iter(dataloader)))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:34:18.941646Z","iopub.execute_input":"2024-07-13T15:34:18.942085Z","iopub.status.idle":"2024-07-13T15:34:18.949355Z","shell.execute_reply.started":"2024-07-13T15:34:18.942050Z","shell.execute_reply":"2024-07-13T15:34:18.948094Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"['anoth' 'one']\ntensor([[1, 1]])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(vectorizer.get_feature_names_out()[:10]) \nprint(next(iter(dataloader))[0, :10])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T15:33:58.795848Z","iopub.execute_input":"2024-07-13T15:33:58.796309Z","iopub.status.idle":"2024-07-13T15:33:58.807257Z","shell.execute_reply.started":"2024-07-13T15:33:58.796274Z","shell.execute_reply":"2024-07-13T15:33:58.805684Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"['anoth' 'one']\ntensor([1, 1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Shakespearean language preprocessing pipeline**\n\nOver at PyBooks, the team wants to transform a vast library of Shakespearean text data for further analysis. The most efficient way to do this is with a text processing pipeline, starting with the preprocessing steps.\n\nThe Shakespearean text data is saved as shakespeare and the sentences have already been extracted.\n\nCreate a list of unique English stopwords, saving to them to stop_words.\n\nInitialize the basic_english tokenizer from torch, and PorterStemmer from nltk.\nComplete the preprocess_sentences() function to enable tokenization, stop word removal, and stemming.","metadata":{}},{"cell_type":"code","source":"shakespeare = [\n    \"To be, or not to be: that is the question.\",\n    \"All the world's a stage, and all the men and women merely players.\",\n    \"A horse! a horse! my kingdom for a horse!\",\n    \"Some are born great, some achieve greatness, and some have greatness thrust upon 'em.\",\n    \"The lady doth protest too much, methinks.\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:33:03.190230Z","iopub.execute_input":"2024-07-13T14:33:03.190779Z","iopub.status.idle":"2024-07-13T14:33:03.198106Z","shell.execute_reply.started":"2024-07-13T14:33:03.190734Z","shell.execute_reply":"2024-07-13T14:33:03.196410Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create a list of stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n# Initialize the tokenizer and stemmer\ntokenizer = get_tokenizer(\"basic_english\")\nstemmer = PorterStemmer() \n\n# Complete the function to preprocess sentences\ndef preprocess_sentences(sentences):\n    processed_sentences = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        tokens = tokenizer(sentence)\n        tokens = [token for token in tokens if token not in stop_words]\n        tokens = [stemmer.stem(token) for token in tokens]\n        processed_sentences.append(' '.join(tokens))\n    return processed_sentences\n\nprocessed_shakespeare = preprocess_sentences(shakespeare)\nprint(processed_shakespeare[:5]) ","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:33:28.084113Z","iopub.execute_input":"2024-07-13T14:33:28.084570Z","iopub.status.idle":"2024-07-13T14:33:28.102541Z","shell.execute_reply.started":"2024-07-13T14:33:28.084533Z","shell.execute_reply":"2024-07-13T14:33:28.101191Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[', question .', \"world ' stage , men women mere player .\", 'hors ! hors ! kingdom hors !', \"born great , achiev great , great thrust upon ' em .\", 'ladi doth protest much , methink .']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(processed_shakespeare)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:34:09.135230Z","iopub.execute_input":"2024-07-13T14:34:09.135685Z","iopub.status.idle":"2024-07-13T14:34:09.142055Z","shell.execute_reply.started":"2024-07-13T14:34:09.135650Z","shell.execute_reply":"2024-07-13T14:34:09.140621Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[', question .', \"world ' stage , men women mere player .\", 'hors ! hors ! kingdom hors !', \"born great , achiev great , great thrust upon ' em .\", 'ladi doth protest much , methink .']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You have successfully preprocessed the sentences and prepared them for encoding. Now you have a clean and transformed dataset to work with for the next step","metadata":{}},{"cell_type":"markdown","source":"# Shakespearean language encoder\n\nWith the preprocessed Shakespearean text at your fingertips, you now need to encode it into a numerical representation. You will need to define the encoding steps before putting the pipeline together. To better handle large amounts of data and efficiently perform the encoding, you will use PyTorch's Dataset and DataLoader for batching and shuffling the data.\n\n* Define a ShakespeareDataset dataset class and complete the __init__ and __getitem__ methods.\n* Complete the encode_sentences() function to take in a list of sentences and encode them using the bag-of-words technique from sklearn.\n* Complete and call the text_processing_pipeline() function by using preprocess_sentences(), encode_sentences(), ShakespeareDataset class, and DataLoader.\n* Print the first ten feature names with the get_feature_names_out() method and components of the first item of dataloader.","metadata":{}},{"cell_type":"code","source":"# Define your Dataset class\nclass ShakespeareDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:47:07.681307Z","iopub.execute_input":"2024-07-13T14:47:07.681801Z","iopub.status.idle":"2024-07-13T14:47:07.688543Z","shell.execute_reply.started":"2024-07-13T14:47:07.681761Z","shell.execute_reply":"2024-07-13T14:47:07.687093Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Complete the encoding function\ndef encode_sentences(sentences):\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(sentences)\n    return X.toarray(), vectorizer\n  \n# Complete the text processing pipeline\ndef text_processing_pipeline(sentences):\n    processed_sentences = preprocess_sentences(sentences)\n    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n    dataset = ShakespeareDataset(encoded_sentences)\n    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n    return dataloader, vectorizer\n\ndataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n\n# Print the vectorizer's feature names and the first 10 components of the first item\nprint(vectorizer.get_feature_names_out()[:10]) \nprint(next(iter(dataloader))[0, :10])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T14:48:25.918953Z","iopub.execute_input":"2024-07-13T14:48:25.920144Z","iopub.status.idle":"2024-07-13T14:48:26.020652Z","shell.execute_reply.started":"2024-07-13T14:48:25.920102Z","shell.execute_reply":"2024-07-13T14:48:26.019389Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['achiev' 'born' 'doth' 'em' 'great' 'hor' 'kingdom' 'ladi' 'men' 'mere']\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You have successfully encoded the Shakespearean text data, and made it useful for your publishing company. The first ten feature representations of the first sentence in your batched data provides a numerical representation, enabling analysis and modeling of the Shakespearean language","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import Dataset, DataLoader\n\n# Ensure nltk resources are downloaded\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Create a list of stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n# Initialize the tokenizer and stemmer\ntokenizer = word_tokenize\nstemmer = PorterStemmer()\n\ndef preprocess_sentences(sentences):\n    processed_sentences = []\n    \n    for sentence in sentences:\n        sentence = sentence.lower()\n        tokens = tokenizer(sentence)\n        tokens = [token for token in tokens if token not in stop_words and token.isalpha()]  # Ensure only alphabetic tokens are retained\n        tokens = [stemmer.stem(token) for token in tokens if token]  # Ensure tokens are not empty after stemming\n        \n        if tokens:  # Check if there are tokens remaining\n            processed_sentences.append(' '.join(tokens))\n        else:\n            processed_sentences.append('')  # Handle case where all tokens are removed\n        \n    print(\"Processed sentences:\", processed_sentences)  # Debug print\n    return processed_sentences\n\ndef encode_sentences(sentences):\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(sentences)\n    encoded_sentences = X.toarray()\n    return encoded_sentences, vectorizer\n\ndef extract_sentences(data):\n    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n    return sentences\n\nclass TextDataset(Dataset):\n    def __init__(self, text):\n        self.text = text\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        return self.text[idx]\n\ndef text_processing_pipeline(text):\n    tokens = preprocess_sentences(text)\n    encoded_sentences, vectorizer = encode_sentences(tokens)\n    dataset = TextDataset(encoded_sentences)\n    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n    return dataloader, vectorizer\n\n# Applying the text processing pipeline\ntext_data = \"This is the first text data. And here is another one.\"\nsentences = extract_sentences(text_data)\n\ndataloaders = []\nvectorizers = []\n\nfor text in sentences:\n    dataloader, vectorizer = text_processing_pipeline([text])  # Pass as list of one sentence\n    dataloaders.append(dataloader)\n    vectorizers.append(vectorizer)\n\nfor vectorizer in vectorizers:\n    print(\"Feature names:\", vectorizer.get_feature_names_out()) \n\nfor dataloader in dataloaders:\n    print(\"Next batch:\", next(iter(dataloader)))\n","metadata":{},"execution_count":null,"outputs":[]}]}