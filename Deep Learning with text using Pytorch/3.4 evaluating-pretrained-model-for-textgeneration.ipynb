{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchmetrics.text import BLEUScore, ROUGEScore","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-06T04:10:48.225283Z","iopub.execute_input":"2024-08-06T04:10:48.226345Z","iopub.status.idle":"2024-08-06T04:10:54.770659Z","shell.execute_reply.started":"2024-08-06T04:10:48.226308Z","shell.execute_reply":"2024-08-06T04:10:54.769745Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating pretrained text generation model\n\nThe PyBooks team has used a pre-trained GPT-2 model that you experimented to generate a text based on a given prompt. Now, they want to evaluate the quality of this generated text. To achieve this, they have tasked you to evaluate generated text using a reference text.\n\n* Begin by initializing the two metrics (BLEU and ROUGE) provided from torchmetrics.text.\n* Use these initialized metrics to calculate the scores between the generated text and the reference text.\n* Display the calculated BLEU and ROUGE scores.\n\n\n","metadata":{}},{"cell_type":"code","source":"reference_text = \"Once upon a time, there was a little girl who lived in a village near the forest.\"\ngenerated_text = \"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\"\n\n# Initialize BLEU and ROUGE scorers\nbleu = BLEUScore()\nrouge = ROUGEScore()\n\n# Calculate the BLEU and ROUGE scores\nbleu_score = bleu([generated_text], [[reference_text]])\nrouge_score = rouge([generated_text], [[reference_text]])\n\n# Print the BLEU and ROUGE scores\nprint(\"BLEU Score:\", bleu_score.item())\nprint(\"ROUGE Score:\\n\", rouge_score)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T04:11:23.663392Z","iopub.execute_input":"2024-08-06T04:11:23.663804Z","iopub.status.idle":"2024-08-06T04:11:23.685246Z","shell.execute_reply.started":"2024-08-06T04:11:23.663774Z","shell.execute_reply":"2024-08-06T04:11:23.684215Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"BLEU Score: 0.08170417696237564\nROUGE Score:\n {'rouge1_fmeasure': tensor(0.2692), 'rouge1_precision': tensor(0.2000), 'rouge1_recall': tensor(0.4118), 'rouge2_fmeasure': tensor(0.1600), 'rouge2_precision': tensor(0.1176), 'rouge2_recall': tensor(0.2500), 'rougeL_fmeasure': tensor(0.2692), 'rougeL_precision': tensor(0.2000), 'rougeL_recall': tensor(0.4118), 'rougeLsum_fmeasure': tensor(0.2692), 'rougeLsum_precision': tensor(0.2000), 'rougeLsum_recall': tensor(0.4118)}\n","output_type":"stream"}]},{"cell_type":"markdown","source":" You've successfully calculated the BLEU and ROUGE scores for the text generation gpt2 model.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}